{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Read the file into a DataFrame: df\n",
    "df = pd.read_csv('dob_job_application_filings_subset.csv')\n",
    "\n",
    "# Print the head of df\n",
    "print(df.head())\n",
    "\n",
    "# Print the tail of df\n",
    "print(df.tail())\n",
    "\n",
    "# Print the shape of df\n",
    "print(df.shape)\n",
    "\n",
    "# Print the columns of df\n",
    "print(df.columns)\n",
    "\n",
    "# Print the head and tail of df_subset\n",
    "print(df.head())\n",
    "print(df.tail())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the info of df\n",
    "print(df.info())\n",
    "\n",
    "# Print the info of df_subset\n",
    "print(df.info())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "# OUTPUT : \n",
    "#                   Job #         Doc #  initial_cost  total_est_fee  \\\n",
    "# count  1.284600e+04  12846.000000  1.284600e+04   1.284600e+04   \n",
    "# mean   2.426788e+08      1.162930  1.803139e+05   2.894909e+03   \n",
    "# std    1.312507e+08      0.514937  7.961524e+06   1.213534e+05   \n",
    "# min    1.036438e+08      1.000000  0.000000e+00   0.000000e+00   \n",
    "# 25%    1.216206e+08      1.000000  0.000000e+00   2.250000e+02   \n",
    "# 50%    2.202645e+08      1.000000  1.597500e+04   4.195000e+02   \n",
    "# 75%    3.208652e+08      1.000000  6.790500e+04   9.297000e+02   \n",
    "# max    5.400246e+08      9.000000  9.003000e+08   1.014000e+07   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# PREMISE : \n",
    "#     Frequency counts for categorical data\n",
    "# As you've seen, .describe() can only be used on numeric columns. So how can you diagnose data issues when you have categorical data? One way is by using the .value_counts() method, which returns the frequency counts for each unique value in a column!\n",
    "# This method also has an optional parameter called dropna which is True by default. What this means is if you have missing data in a column, it will not give a frequency count of them. You want to set the dropna column to False so if there are missing values in a column, it will give you the frequency counts.\n",
    "# In this exercise, you're going to look at the 'Borough', 'State', and 'Site Fill' columns to make sure all the values in there are valid. When looking at the output, do a sanity check: Are all values in the 'State' column from NY, for example? Since the dataset consists of applications filed in NY, you would expect this to be the case.\n",
    "\n",
    "# Print the value counts for 'Borough'\n",
    "print(df['Borough'].value_counts(dropna=False))\n",
    "\n",
    "# Print the value_counts for 'State'\n",
    "print(df['State'].value_counts(dropna=False))\n",
    "\n",
    "# Print the value counts for 'Site Fill'\n",
    "print(df['Site Fill'].value_counts(dropna=False))\n",
    "\n",
    "# OUTPUT : \n",
    "#     MANHATTAN        6310\n",
    "# BROOKLYN         2866\n",
    "# QUEENS           2121\n",
    "# BRONX             974\n",
    "# STATEN ISLAND     575\n",
    "# Name: Borough, dtype: int64\n",
    "# NY    12391\n",
    "# NJ      241\n",
    "# PA       38\n",
    "# CA       20\n",
    "# OH       19\n",
    "# IL       17\n",
    "# FL       17\n",
    "# CT       16\n",
    "# TX       13\n",
    "# TN       10\n",
    "# DC        7\n",
    "# MD        7\n",
    "# GA        6\n",
    "# MA        6\n",
    "# KS        6\n",
    "# VA        5\n",
    "# CO        4\n",
    "# SC        3\n",
    "# AZ        3\n",
    "# WI        3\n",
    "# MN        3\n",
    "# NC        2\n",
    "# RI        2\n",
    "# UT        2\n",
    "# MI        1\n",
    "# IN        1\n",
    "# WA        1\n",
    "# NM        1\n",
    "# VT        1\n",
    "# Name: State, dtype: int64\n",
    "# NOT APPLICABLE                              7806\n",
    "# NaN                                         4205\n",
    "# ON-SITE                                      519\n",
    "# OFF-SITE                                     186\n",
    "# USE UNDER 300 CU.YD                          130\n",
    "# Name: Site Fill, dtype: int64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import matplotlib.pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the histogram\n",
    "df['Existing Zoning Sqft'].plot(kind='hist', rot=70, logx=True, logy=True)\n",
    "\n",
    "# Display the histogram\n",
    "plt.show()\n",
    "\n",
    "#Here kind = kind of plot you want\n",
    "#logx = to shrink the plot\n",
    "#rot = ???\n",
    "\n",
    "# Import necessary modules\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the boxplot\n",
    "df.boxplot(column='initial_cost', by='Borough', rot=90)\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "###Here we are displaying initial cost per borough!\n",
    "##This also shows the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "######MELTING AND PIVOTING\n",
    "# Print the head of airquality\n",
    "print(airquality.head())\n",
    "\n",
    "# Melt airquality: airquality_melt\n",
    "airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'])\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())\n",
    "\n",
    "###PREMISE\n",
    "# In this exercise, you will practice melting a DataFrame using pd.melt(). \n",
    "# There are two parameters you should be aware of: id_vars and value_vars. \n",
    "# The id_vars represent the columns of the data you do not want to melt (i.e., keep it in its current shape),\n",
    "# while the value_vars represent the columns you do wish to melt into rows. \n",
    "# By default, if no value_vars are provided, all columns not set in the id_vars will be melted. \n",
    "# This could save a bit of typing, depending on the number of columns that need to be melted.\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality.head())\n",
    "\n",
    "# Melt airquality: airquality_melt\n",
    "airquality_melt = pd.melt(airquality, id_vars=['Month', 'Day'], var_name='measurement', value_name='reading')\n",
    "\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())\n",
    "\n",
    "###Above we are just renaming the melted columns. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###PIVOT_TABLE\n",
    "# Print the head of airquality_melt\n",
    "print(airquality_melt.head())\n",
    "\n",
    "# Pivot airquality_melt: airquality_pivot\n",
    "airquality_pivot = airquality_melt.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading')\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())\n",
    "\n",
    "###This is opposite of melt. Pivot_table \n",
    "\n",
    "####Resetting the index of a dataframe\n",
    "# Print the index of airquality_pivot\n",
    "print(airquality_pivot.index)\n",
    "\n",
    "# Reset the index of airquality_pivot: airquality_pivot\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the new index of airquality_pivot\n",
    "print(airquality_pivot.index)\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())\n",
    "\n",
    "\n",
    "\n",
    "####PIVOTING DUPLICATE VALUES USING NP.MEAN\n",
    "# Pivot airquality_dup: airquality_pivot\n",
    "airquality_pivot = airquality_dup.pivot_table(index=['Month', 'Day'], columns='measurement', values='reading', aggfunc=np.mean)\n",
    "\n",
    "# Reset the index of airquality_pivot\n",
    "airquality_pivot = airquality_pivot.reset_index()\n",
    "\n",
    "# Print the head of airquality_pivot\n",
    "print(airquality_pivot.head())\n",
    "\n",
    "# Print the head of airquality\n",
    "print(airquality.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "####CREATING A NEW COLUMN WITH STRING MANIPULATION FROM ANOTHER COLUMN\n",
    "# Melt tb: tb_melt\n",
    "tb_melt = pd.melt(tb, id_vars=['country', 'year'])\n",
    "\n",
    "# Create the 'gender' column\n",
    "tb_melt['gender'] = tb_melt.variable.str[0]\n",
    "\n",
    "# Create the 'age_group' column\n",
    "tb_melt['age_group'] = tb_melt.variable.str[1:]\n",
    "\n",
    "# Print the head of tb_melt\n",
    "print(tb_melt.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###### SPLITTING A COLUMN WITH SPLIT AND GET METHODS\n",
    "# Melt ebola: ebola_melt\n",
    "ebola_melt = pd.melt(ebola, id_vars=['Date', 'Day'], var_name='type_country', value_name='counts')\n",
    "\n",
    "# Create the 'str_split' column\n",
    "ebola_melt['str_split'] = ebola_melt.type_country.str.split('_') ##Notice how we access the str attribute of column\n",
    "\n",
    "# Create the 'type' column\n",
    "ebola_melt['type'] = ebola_melt.str_split.str.get(0)\n",
    "\n",
    "# Create the 'country' column\n",
    "ebola_melt['country'] = ebola_melt.str_split.str.get(1)\n",
    "\n",
    "# Print the head of ebola_melt\n",
    "print(ebola_melt.head())\n",
    "\n",
    "\n",
    "#######CONCATENATING DATAFRAMES\n",
    "    # Concatenate uber1, uber2, and uber3: row_concat\n",
    "row_concat = pd.concat([uber1,uber2,uber3])  ##<----REMEMBER TO PASS DATAFRAMES AS LIST\n",
    "# Print the shape of row_concat\n",
    "print(row_concat.shape)\n",
    "# Print the head of row_concat\n",
    "print(row_concat.head)\n",
    "\n",
    "\n",
    "###### CONCATENATING SIDEWAYS : use axis = 1\n",
    "# Concatenate ebola_melt and status_country column-wise: ebola_tidy\n",
    "ebola_tidy = pd.concat([ebola_melt,status_country],axis=1)\n",
    "\n",
    "# Print the shape of ebola_tidy\n",
    "print(ebola_tidy.shape)\n",
    "\n",
    "# Print the head of ebola_tidy\n",
    "print(ebola_tidy.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########  READING IN FILES THAT MATCH A PATTERN \n",
    "# Import necessary modules\n",
    "import glob               ## this helps in pattern match fetching\n",
    "import pandas as pd\n",
    "\n",
    "# Write the pattern: pattern\n",
    "pattern = '*.csv'\n",
    "\n",
    "# Save all file matches: csv_files\n",
    "csv_files = glob.glob(pattern)\n",
    "\n",
    "# Print the file names\n",
    "print(csv_files)\n",
    "\n",
    "# Load the second file into a DataFrame: csv2\n",
    "csv2 = pd.read_csv(csv_files[1])\n",
    "\n",
    "# Print the head of csv2\n",
    "print(csv2.head())\n",
    "\n",
    "\n",
    "\n",
    "####### APPENDING THE FOUND FILES INTO A SINGLE DATAFRAME\n",
    "# Create an empty list: frames\n",
    "frames = []\n",
    "#  Iterate over csv_files\n",
    "for csv in csv_files:\n",
    "    #  Read csv into a DataFrame: df\n",
    "    df = pd.read_csv(csv)    \n",
    "    # Append df to frames\n",
    "    frames.append(df)\n",
    "# Concatenate frames into a single DataFrame: uber\n",
    "uber = pd.concat(frames)\n",
    "# Print the shape of uber\n",
    "print(uber.shape)\n",
    "# Print the head of uber\n",
    "print(uber.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "############### MERGING DATAFRAMES #########\n",
    "##1 TO 1\n",
    "# Merge the DataFrames: o2o\n",
    "o2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "# Print o2o\n",
    "print(o2o)\n",
    "\n",
    "##Many To One\n",
    "# Merge the DataFrames: m2o\n",
    "m2o = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "# Print m2o\n",
    "print(m2o)\n",
    "\n",
    "##Many To Many\n",
    "# Merge site and visited: m2m\n",
    "m2m = pd.merge(left=site, right=visited, left_on='name', right_on='site')\n",
    "# Merge m2m and survey: m2m\n",
    "m2m = pd.merge(left=m2m, right=survey, left_on='ident', right_on='taken')\n",
    "# Print the first 20 lines of m2m\n",
    "print(m2m.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "################ CONVERT DATA TYPES\n",
    "# Convert the sex column to type 'category'\n",
    "tips.sex = tips.sex.astype('category')\n",
    "# Convert the smoker column to type 'category'\n",
    "tips.smoker = tips.smoker.astype('category')\n",
    "# Print the info of tips\n",
    "print(tips.info())        ## Remember that less memory is used if proper variables are used\n",
    "\n",
    "#######WORKING WITH NUMERIC DATA\n",
    "# Convert 'total_bill' to a numeric dtype\n",
    "tips['total_bill'] = pd.to_numeric(tips['total_bill'], errors='coerce')\n",
    "# Convert 'tip' to a numeric dtype\n",
    "tips['tip'] = pd.to_numeric(tips['tip'], errors='coerce')\n",
    "# Print the info of tips\n",
    "print(tips.info())                           ##Remember that coerce makes it convert as NaN that otherwise wont have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######  STRING PARSING WITH REGULAR EXPRESSIONS\n",
    "# Import the regular expression module\n",
    "import re\n",
    "# Compile the pattern: prog\n",
    "prog = re.compile('\\d{3}-\\d{3}-\\d{4}')\n",
    "# See if the pattern matches\n",
    "result = prog.match('123-456-7890')\n",
    "print(bool(result))\n",
    "# See if the pattern matches\n",
    "result = bool(prog.match('1123-456-7890'))\n",
    "print(bool(result))\n",
    "\n",
    "\n",
    "########## EXTRACTING NUMERICAL VALUE FROM STRINGS\n",
    "# Import the regular expression module\n",
    "import re\n",
    "# Find the numeric values: matches\n",
    "matches = re.findall('\\d+', 'the recipe calls for 10 strawberries and 1 banana')   ###Note here that the + makes sure that \n",
    "                                                                        ###we take 10 as 10 and not as 1,0. \n",
    "# Print the matches\n",
    "print(matches)\n",
    "\n",
    "\n",
    "#########  MORE PATTERN MATCHES\n",
    "# Write the first pattern\n",
    "pattern1 = bool(re.match(pattern='\\d{3}-\\d{3}-\\d{4}', string='123-456-7890'))\n",
    "print(pattern1)\n",
    "# Write the second pattern\n",
    "pattern2 = bool(re.match(pattern='\\$\\d{3}\\.\\d{2}', string='$123.45'))\n",
    "print(pattern2)\n",
    "# Write the third pattern\n",
    "pattern3 = bool(re.match(pattern='[A-Z]\\w*', string='Australia'))\n",
    "print(pattern3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########  CUSTOM FUNCTIONS TO CLEAN DATA\n",
    "\n",
    "# Define recode_sex()\n",
    "def recode_sex(sex_value):\n",
    "    # Return 1 if sex_value is 'Male'\n",
    "    if sex_value == 'Male':\n",
    "        return 1\n",
    "    # Return 0 if sex_value is 'Female'    \n",
    "    elif sex_value == 'Female':\n",
    "        return 0\n",
    "    # Return np.nan    \n",
    "    else:\n",
    "        return np.nan\n",
    "# Apply the function to the sex column\n",
    "tips['sex_recode'] = tips['sex'].apply(recode_sex)  ###Notice how recode sex was applied to the column sex. \n",
    "# Print the first five rows of tips\n",
    "print(tips.head())\n",
    "\n",
    "######### LAMBDA FUNCTIONS\n",
    "import re\n",
    "# Write the lambda function using replace\n",
    "tips['total_dollar_replace'] = tips.total_dollar.apply(lambda x: x.replace('$', ''))\n",
    "# Write the lambda function using regular expressions\n",
    "tips['total_dollar_re'] = tips.total_dollar.apply(lambda x: re.findall('\\d+\\.\\d+', x)[0])  ##notice it is re.find... not x.fi\n",
    "# Print the head of tips\n",
    "print(tips.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#########  DROPPING DUPLICATE ROWS\n",
    "# Create the new DataFrame: tracks\n",
    "tracks = billboard[['year','artist','track','time']]\n",
    "# Print info of tracks\n",
    "print(tracks.info())\n",
    "# Drop the duplicates: tracks_no_duplicates\n",
    "tracks_no_duplicates = tracks.drop_duplicates()  ### <--- This is the main method\n",
    "# Print info of tracks\n",
    "print(tracks_no_duplicates.info())\n",
    "\n",
    "\n",
    "####### FILLING MISSING COLUMNS WITH MEAN\n",
    "# Calculate the mean of the Ozone column: oz_mean\n",
    "oz_mean = airquality.Ozone.mean()\n",
    "# Replace all the missing values in the Ozone column with the mean\n",
    "airquality['Ozone'] = airquality.Ozone.fillna(oz_mean)\n",
    "# Print the info of airquality\n",
    "print(airquality.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########  TESTING DATA WITH ASSERT STATEMENT\n",
    "import pandas as pd\n",
    "# Assert that there are no missing values\n",
    "assert pd.notnull(ebola).all().all()    ### This .all() first gives o/p for columns and then for table itself.\n",
    "# Assert that all values are >= 0\n",
    "assert (ebola >= 0).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################   FINAL EXERCISE   #######################################\n",
    "# Import matplotlib.pyplot                    g1800s dataset\n",
    "import matplotlib.pyplot as plt\n",
    "# Create the scatter plot\n",
    "g1800s.plot(kind='scatter', x='1800', y='1899')\n",
    "# Specify axis labels\n",
    "plt.xlabel('Life Expectancy by Country in 1800')\n",
    "plt.ylabel('Life Expectancy by Country in 1899')\n",
    "# Specify axis limits\n",
    "plt.xlim(20, 55)\n",
    "plt.ylim(20, 55)\n",
    "# Display the plot\n",
    "plt.show()\n",
    "\n",
    "\n",
    "###################### WRITING A FUNCTION TO VALIDATE BY COLUMN\n",
    "def check_null_or_valid(row_data):\n",
    "    \"\"\"Function that takes a row of data,\n",
    "    drops all missing values,\n",
    "    and checks if all remaining values are greater than or equal to 0\n",
    "    \"\"\"\n",
    "    no_na = row_data.dropna()[1:-1]\n",
    "    numeric = pd.to_numeric(no_na)\n",
    "    ge0 = numeric >= 0\n",
    "    return ge0\n",
    "# Check whether the first column is 'Life expectancy'\n",
    "assert g1800s.columns[0] == 'Life expectancy'\n",
    "# Check whether the values in the row are valid\n",
    "assert g1800s.iloc[:, 1:].apply(check_null_or_valid, axis=1).all().all()\n",
    "# Check that there is only one instance of each country\n",
    "assert g1800s['Life expectancy'].value_counts()[0] == 1   ##Only checking first value as the highest frequency value is printer first\n",
    "\n",
    "\"\"\"\n",
    "Define a function called check_null_or_valid() that takes in one argument: row_data.\n",
    "Inside the function, convert no_na to a numeric data type using pd.to_numeric().\n",
    "Write an assert statement to make sure the first column (index 0) of the g1800s DataFrame is 'Life expectancy'.\n",
    "Write an assert statement to test that all the values are valid for the g1800s DataFrame. Use the check_null_or_valid() function placed inside the .apply() method for this. Note that because you're applying it over the entire DataFrame, and not just one column, you'll have to chain the .all() method twice, and remember that you don't have to use () for functions placed inside .apply().\n",
    "Write an assert statement to make sure that each country occurs only once in the data. Use the .value_counts() method on the 'Life expectancy' column for this. Specifically, index 0 of .value_counts() will contain the most frequently occuring value. If this is equal to 1 for the 'Life expectancy' column, then you can be certain that no country appears more than once in the data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "##################### CONCAT THE DATAFRAMES\n",
    "# Concatenate the DataFrames row-wise\n",
    "gapminder = pd.concat([g1800s,g1900s,g2000s], axis=0)\n",
    "# Print the shape of gapminder\n",
    "print(gapminder.shape)\n",
    "# Print the head of gapminder\n",
    "print(gapminder.head())\n",
    "\n",
    "\n",
    "################### RESHAPING THE DATA\n",
    "# Melt gapminder: gapminder_melt\n",
    "gapminder_melt = pd.melt(gapminder,id_vars=['Life expectancy'])\n",
    "# Rename the columns\n",
    "gapminder_melt.columns = ['country','year','Life expectancy']   #<----Renaming the columns\n",
    "# Print the head of gapminder_melt\n",
    "print(gapminder_melt.head())\n",
    "\n",
    "\n",
    "#################### CHECKING DATA TYPES & CONVERTING\n",
    "# Convert the year column to numeric\n",
    "gapminder.year = pd.to_numeric(gapminder.year)\n",
    "# Test if country is of type object\n",
    "assert gapminder.country.dtypes == np.object\n",
    "# Test if year is of type int64\n",
    "assert gapminder.year.dtypes == np.int64\n",
    "# Test if life_expectancy is of type float64\n",
    "assert gapminder.life_expectancy.dtypes == np.float64\n",
    "\n",
    "################### LOOKING AT COUTRY SPELLINGS \n",
    "# Create the series of countries: countries\n",
    "countries = gapminder.country\n",
    "# Drop all the duplicates from countries\n",
    "countries = countries.drop_duplicates()\n",
    "# Write the regular expression: pattern\n",
    "pattern = '^[A-Za-z\\.\\s]*$'\n",
    "# Create the Boolean vector: mask\n",
    "mask = countries.str.contains(pattern)\n",
    "# Invert the mask: mask_inverse\n",
    "mask_inverse = ~mask\n",
    "# Subset countries using mask_inverse: invalid_countries\n",
    "invalid_countries = countries.loc[mask_inverse]\n",
    "# Print invalid_countries\n",
    "print(invalid_countries)\n",
    "\n",
    "\n",
    "\n",
    "################### MORE DATA CLEANING AND PROCESSING \n",
    "# Assert that country does not contain any missing values\n",
    "assert pd.notnull(gapminder.country).all()\n",
    "# Assert that year does not contain any missing values\n",
    "assert pd.notnull(gapminder.year).all()\n",
    "# Drop the missing values\n",
    "gapminder = gapminder.dropna()    #<------This one has many formats\n",
    "# Print the shape of gapminder\n",
    "print(gapminder.shape)\n",
    "\n",
    "\n",
    "#################  WRAPPING UP ~ ALSO SAVING AS CSV\n",
    "# Add first subplot\n",
    "plt.subplot(2, 1, 1) \n",
    "# Create a histogram of life_expectancy\n",
    "gapminder.life_expectancy.plot(kind='hist')\n",
    "# Group gapminder: gapminder_agg\n",
    "gapminder_agg = gapminder.groupby('year')['life_expectancy'].mean()\n",
    "# Print the head of gapminder_agg\n",
    "print(gapminder_agg.head())\n",
    "# Print the tail of gapminder_agg\n",
    "print(gapminder_agg.tail())\n",
    "# Add second subplot\n",
    "plt.subplot(2, 1, 2)\n",
    "# Create a line plot of life expectancy per year\n",
    "gapminder_agg.plot()\n",
    "# Add title and specify axis labels\n",
    "plt.title('Life expectancy over the years')\n",
    "plt.ylabel('Life expectancy')\n",
    "plt.xlabel('Year')\n",
    "# Display the plots\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
